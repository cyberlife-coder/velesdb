# ğŸ“Š VelesDB Scale Analysis: 50M Vectors

> **Analysis Date**: December 30, 2025  
> **Author**: Wiscale France (Julien Lange)  
> **Status**: Architecture Review

---

## ğŸ¯ Objective

Compare VelesDB search latency against competitors at **50M vectors scale** and identify architectural hotspots.

## ğŸ“‹ Current Comparison (README)

| Database | Scale | Latency | Notes |
|----------|-------|---------|-------|
| **VelesDB** | 10K | **128Âµs** | Local Criterion benchmark |
| Qdrant | 50M | ~30ms | From public benchmarks |
| pgvectorscale | 50M | ~31ms | From Timescale benchmarks |
| pgvector | 50M | ~50ms | From public benchmarks |

âš ï¸ **Issue**: This comparison is misleading â€” VelesDB is tested at 10K, competitors at 50M.

---

## ğŸ”¬ Architecture Analysis

### Current VelesDB Architecture (v0.5.1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VelesDB Single Node                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  VelesQL    â”‚â”€â”€â”€â”€â–¶â”‚         Collection              â”‚    â”‚
â”‚  â”‚  Parser     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚       HNSW Index           â”‚ â”‚    â”‚
â”‚                      â”‚  â”‚  (hnsw_rs library)         â”‚ â”‚    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚  - In-memory graph         â”‚ â”‚    â”‚
â”‚  â”‚  REST API   â”‚â”€â”€â”€â”€â–¶â”‚  â”‚  - RwLock<ManuallyDrop>    â”‚ â”‚    â”‚
â”‚  â”‚  (Axum)     â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                                  â”‚    â”‚
â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚                      â”‚  â”‚    Vector Storage          â”‚ â”‚    â”‚
â”‚                      â”‚  â”‚  - FxHashMap (in RAM)      â”‚ â”‚    â”‚
â”‚                      â”‚  â”‚  - MmapStorage (disk)      â”‚ â”‚    â”‚
â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Requirements at 50M Scale

| Component | Calculation | Size |
|-----------|-------------|------|
| **Vectors** | 50M Ã— 768D Ã— 4 bytes | **~143 GB** |
| **HNSW Graph** | 50M Ã— M Ã— 2 Ã— 8 bytes (M=24) | **~18 GB** |
| **ID Mappings** | 50M Ã— 16 bytes | **~760 MB** |
| **FxHashMap overhead** | ~40% | **~65 GB** |
| **Total RAM Required** | | **~230 GB** |

âš ï¸ **Conclusion**: VelesDB at 50M requires a high-memory server (~256GB RAM).

---

## âš¡ Projected Performance at 50M

### Theoretical HNSW Complexity

- **Search**: O(log n Ã— ef_search Ã— distance_calc)
- **At 10K**: ~5 layers, ~128 distance calcs
- **At 50M**: ~8-9 layers, ~256+ distance calcs

### Projected Latency Estimate

```
Search latency = layers Ã— candidates Ã— distance_time + cache_miss_penalty

At 10K (current):
  = 5 Ã— 128 Ã— 41ns + minimal_cache_miss
  = ~65Âµs + overhead
  = ~128Âµs measured âœ“

At 50M (projected):
  = 9 Ã— 256 Ã— 41ns + significant_cache_miss_penalty
  = ~94Âµs base + ~2-10ms cache miss penalty
  = ~2-15ms projected
```

### Expected Performance Range

| Scale | VelesDB (projected) | Qdrant | pgvectorscale |
|-------|---------------------|--------|---------------|
| 10K | **128Âµs** âœ“ | ~1ms | ~5ms |
| 1M | **~500Âµs - 2ms** | ~5ms | ~10ms |
| 10M | **~2-5ms** | ~15ms | ~20ms |
| 50M | **~5-15ms** | ~30ms | ~31ms |
| 100M | **~10-30ms** | ~50ms | N/A |

**Hypothesis**: VelesDB should be **2-3x faster** than competitors at same scale due to:
- SIMD-optimized distance calculations (41ns vs ~100ns)
- No network overhead (single binary)
- No container/VM overhead

---

## ğŸ”¥ Identified Hotspots at 50M Scale

### 1. **Memory Bandwidth** (Critical)
```rust
// Current: FxHashMap stores all vectors in RAM
vectors: RwLock<FxHashMap<usize, Vec<f32>>>,
```
- **Issue**: Random access pattern causes cache misses
- **Impact**: ~100-500ns per cache miss at L3 boundary
- **At 50M**: Working set >> L3 cache â†’ constant cache misses

### 2. **HNSW Graph Traversal** (High)
```rust
// hnsw_rs internal graph structure
Hnsw<'static, f32, DistCosine>
```
- **Issue**: Graph edges scattered in memory
- **Impact**: Each hop = potential cache miss
- **At 50M**: 8-9 layers Ã— random memory access

### 3. **Lock Contention** (Medium)
```rust
inner: RwLock<ManuallyDrop<HnswInner>>,
mappings: RwLock<HnswMappings>,
vectors: RwLock<FxHashMap<usize, Vec<f32>>>,
```
- **Issue**: Multiple RwLocks for read path
- **Impact**: Reader contention at high QPS
- **Mitigation**: Read locks are fast, but still overhead

### 4. **ID Mapping Lookup** (Low-Medium)
```rust
// Each result requires mapping lookup
if let Some(id) = mappings.get_id(n.d_id) {
    results.push((id, score));
}
```
- **Issue**: HashMap lookup per result
- **Impact**: ~10-50ns per lookup
- **At 50M**: Larger hash table = more cache misses

---

## ğŸš€ Optimization Opportunities

### A. Immediate Optimizations (VelesDB Core)

| Optimization | Effort | Impact | Status |
|--------------|--------|--------|--------|
| **SQ8 Quantization** | Low | High | âœ… **Implemented** - `HnswParams::with_sq8()` |
| **Binary Quantization** | Low | High | âœ… **Implemented** - `HnswParams::with_binary()` |
| **Contiguous vector storage** | Medium | High | ğŸ”œ Planned |
| **Prefetch optimization** | Low | Medium | ğŸ”œ Planned |
| **Lock-free reads** | Medium | Medium | ğŸ”œ Planned |

### Usage: SQ8 Quantization (4x Memory Reduction)

```rust
use velesdb_core::index::HnswParams;

// SQ8: 4x memory reduction with ~1% recall loss
let params = HnswParams::with_sq8(768);

// Binary: 32x memory reduction (edge/IoT)
let params = HnswParams::with_binary(768);
```

| Mode | Memory (768D) | Recall Loss | Use Case |
|------|---------------|-------------|----------|
| Full (f32) | 3 KB/vector | 0% | Default, max precision |
| SQ8 (u8) | 776 B/vector | ~1% | Scale, RAM-constrained |
| Binary (1-bit) | 96 B/vector | ~5-10% | Edge, IoT, 32x compression |

### B. VelesDB Premium Opportunities

| Feature | Effort | Impact | Description |
|---------|--------|--------|-------------|
| **Distributed Sharding** | High | Critical | Split 50M across N nodes |
| **GPU Acceleration** | High | High | CUDA/Metal for distance calc |
| **Tiered Storage** | Medium | High | Hot vectors in RAM, cold on SSD |
| **Async Prefetch** | Medium | Medium | Background prefetch during search |

---

## ğŸ“ Recommended Architecture for 50M+

### Option 1: High-Memory Single Node (Current Architecture)
```
Requirements: 256GB+ RAM server
Latency: ~5-15ms at 50M
Cost: $$$ (RAM is expensive)
Complexity: Low
```

### Option 2: Sharded Architecture (Premium)
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Router     â”‚
                    â”‚  (VelesDB)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ Shard 1 â”‚       â”‚ Shard 2 â”‚       â”‚ Shard 3 â”‚
   â”‚  ~17M   â”‚       â”‚  ~17M   â”‚       â”‚  ~17M   â”‚
   â”‚  48GB   â”‚       â”‚  48GB   â”‚       â”‚  48GB   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Requirements: 3Ã— 64GB servers
Latency: ~2-5ms (parallel search + merge)
Cost: $$ (commodity hardware)
Complexity: High
```

### Option 3: Quantization + Compression (Core)
```
SQ8 Quantization: 50M Ã— 768D Ã— 1 byte = ~36GB
Binary Quantization: 50M Ã— 768D Ã— 1 bit = ~4.5GB

Requirements: 64-128GB RAM server
Latency: ~5-20ms (slight recall loss)
Cost: $ (standard server)
Complexity: Low
```

---

## ğŸ¯ Honest README Update

### Recommended Comparison Table

| Metric | ğŸº VelesDB | Qdrant | pgvectorscale |
|--------|-----------|--------|---------------|
| **10K vectors** | **128Âµs** | ~1ms | ~5ms |
| **1M vectors** | **~1ms** | ~5ms | ~10ms |
| **50M vectors** | ~5-15ms* | ~30ms | ~31ms |
| **Architecture** | Single Binary | Container | Postgres Ext |
| **RAM for 50M** | ~230GB | ~150GB | ~100GB |

*Projected estimate â€” requires validation benchmark

### Sweet Spot Messaging

```
VelesDB excels at:
âœ… 1K - 10M vectors (microsecond to low-ms latency)
âœ… Edge/Desktop/WASM deployment
âœ… On-premises with data sovereignty
âœ… Low-resource environments

For 50M+ vectors, consider:
âš ï¸ High-memory single node (256GB+ RAM)
âš ï¸ VelesDB Premium (distributed sharding) - Coming Soon
âš ï¸ Quantization (SQ8) for memory reduction
```

---

## ğŸ§ª Validation Benchmark

### Run the Benchmark Script

A ready-to-use Python script is available: [`benchmarks/benchmark_50m.py`](../benchmarks/benchmark_50m.py)

```bash
# Install dependencies
pip install numpy requests qdrant-client

# Quick test (1M vectors, ~8GB RAM)
python benchmarks/benchmark_50m.py --quick

# Full 50M benchmark (requires 256GB+ RAM)
python benchmarks/benchmark_50m.py --full

# Custom scale
python benchmarks/benchmark_50m.py --vectors 10000000

# VelesDB only (skip Qdrant comparison)
python benchmarks/benchmark_50m.py --quick --velesdb-only
```

### Cloud Environment Setup

For 50M vectors, we recommend:

```bash
# AWS: r6i.8xlarge (256GB RAM, 32 vCPU) ~$2/hour
# Azure: Standard_E64s_v5 (256GB RAM)
# GCP: n2-highmem-64 (256GB RAM)

# Start Qdrant for comparison
docker run -p 6333:6333 qdrant/qdrant

# Run full benchmark
python benchmarks/benchmark_50m.py --full
```

### Output

Results are saved to `benchmark_results.json`:

```json
{
  "config": {"vector_count": 50000000, "dimension": 768},
  "velesdb": {"latency_p50_ms": 8.5, "latency_p99_ms": 15.2},
  "qdrant": {"latency_p50_ms": 28.3, "latency_p99_ms": 45.6}
}
```

---

## ğŸ“ Conclusions

### 1. Is VelesDB Competitive at 50M?

**Yes, with caveats:**
- Projected 2-3x faster than Qdrant/pgvectorscale at same scale
- But requires significant RAM investment (~230GB)
- Single-node architecture limits horizontal scaling

### 2. What Are the Hotspots?

| Priority | Hotspot | Solution |
|----------|---------|----------|
| P0 | Memory bandwidth | Contiguous storage + prefetch |
| P1 | Graph traversal | Better cache locality |
| P2 | Lock contention | Lock-free structures |

### 3. Can Premium Help?

**Yes, significantly:**
- **Distributed sharding** â†’ Horizontal scaling
- **GPU acceleration** â†’ 10-100x distance calc speedup
- **Tiered storage** â†’ Cost reduction

### 4. Architecture Limitation?

**Partially:**
- Single-node is architectural choice for simplicity
- Can scale to ~100M on high-memory hardware
- Beyond 100M, distributed architecture needed (Premium)

---

## ğŸ‡«ğŸ‡· About This Analysis

This analysis is provided by **Wiscale France**, founded by **Julien Lange**.

We believe in **honest benchmarking**:
- Don't compare 10K to 50M
- Show projected numbers with caveats
- Acknowledge architectural limitations

ğŸ“§ Contact: contact@wiscale.fr

---

*Last updated: December 30, 2025*
